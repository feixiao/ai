## 个人工作机选购

时间: 2025年11月17日

+ [《性能测试》](https://github.com/feixiao/llm-benchmark)

#### DS API
+ 平均速度 33.01 tokens/s


#### M4 + M4 Pro
+ 平均速度 在 10-20 token/s
+ [《Mac mini本地部署DeepSeek模型：硬件适配与性能优化指南》](https://comate.baidu.com/zh/page/i97nc1rh6ca)

#### M1 Pro (32GB RAM, 16-core GPU)
+ lucasmg/deepseek-r1-8b-0528-qwen3-q4_K_M-tool-true:latest 13.08 tokens/sec (自己实测)
+ qwen2.5:3b-instruct-q4_K_M                            25.44 tokens/sec (自己实测)  
+ qwen2.5:7b-instruct-q4_K_M                            15.55 tokens/sec (自己实测)
+ Qwen2.5:7B (4bit)                                     26.85 tokens/s
+ Qwen2.5-7B-Instruct (4bit) （MLX models）              38.99 tokens/s
+ deepseek-r1:14b (4bit)        21.16 tokens/s
#### DGX SPARK
+ DeepSeek R1 14B（蒸馏版）​约 83.5 tokens/秒​ 性能较好。在批处理大小为8时，预填充速度可达2074 tokens/秒 。适合中小规模任务的快速原型开发。



#### 参考资料
+ [《Local LLM inference speed tests on various devices》](https://github.com/itsmostafa/inference-speed-tests)