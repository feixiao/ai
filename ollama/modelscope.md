### modelscope模型下载

#### 模型选择
对于 M4 Max 128G，不要止步于 Q4_K_M。你的机器有能力运行 Q6_K 甚至 Q8_0，这能显著减少模型“胡言乱语”的概率，如果内存允许，无脑选择 Q8_0。
+ 7B - 34B (较小模型)，推荐 GGUF 格式Q8_0
+ 70B - 81B (中大型模型)，首选 Q6_K。Q6 提供了接近 Q8 的精度，但能节省更多内存给长上下文（Context）。