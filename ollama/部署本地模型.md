## ollama部署本地模型


### 工具准备
```shell
pip install modelscope
```

### 下载模型
```shell
modelscope download --model Qwen/Qwen3-Embedding-8B-GGUF --local_dir ./models/Qwen3-Embedding-8B-GGUF
```



#### ollama直接运行GGUF模型
+ Modelfile 示例
```shell
# 指定本地GGUF模型文件的路径
FROM /path/to/your/model.gguf
# 设置系统提示词，定义模型角色
SYSTEM "你是一个有用的AI助手。"
# 调整参数，例如控制生成创造性的温度参数
PARAMETER temperature 0.7
```
+ 运行
```shell
ollama create my-model -f ./Modelfile
ollama run my-model
```

#### ollama直接运行MLX模型


#### 模型选择
对于 M4 Max 128G，不要止步于 Q4_K_M。你的机器有能力运行 Q6_K 甚至 Q8_0，这能显著减少模型“胡言乱语”的概率，如果内存允许，无脑选择 Q8_0。
+ 7B - 34B (较小模型)，推荐 GGUF 格式Q8_0
+ 70B - 81B (中大型模型)，首选 Q6_K。Q6 提供了接近 Q8 的精度，但能节省更多内存给长上下文（Context）。
+ 对于 M4 Max，MLX 是性能上限，GGUF 是稳定下限